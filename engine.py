# engine.py ‚Äî High-Performance Parallel Scraper (Fixed Config Passing)
import concurrent.futures
import traceback
import os
from typing import Dict, List

# Import Site Modules
import gogo_mn
import ikon_mn
import news_mn
import ublife_mn
import lemonpress_mn
import caak_mn
import bolortoli_mn

# Define sites config for scalability
SITES_CONFIG = [
    {"module": gogo_mn, "name": "gogo_mn"},
    {"module": ikon_mn, "name": "ikon_mn"},
    {"module": news_mn, "name": "news_mn"},
    {"module": ublife_mn, "name": "ublife_mn"},
    {"module": lemonpress_mn, "name": "lemonpress_mn"},
    {"module": caak_mn, "name": "caak_mn"},
    {"module": bolortoli_mn, "name": "bolortoli_mn"},
]

def _scrape_wrapper(site_conf: Dict) -> Dict[str, List]:
    """
    Single site scraper wrapper to handle errors independently.
    Now properly passes env configurations to scraper functions.
    """
    mod = site_conf["module"]
    name = site_conf["name"]
    results = []
    
    # 1. Environment Variables —É–Ω—à–∏—Ö (Default —É—Ç–≥—É—É–¥—ã–≥ —ç–Ω–¥ —Ç–æ—Ö–∏—Ä—É—É–ª–Ω–∞)
    # Bolor-toli –∑—ç—Ä—ç–≥—Ç –∑–æ—Ä–∏—É–ª–∂ default dwell-–∏–π–≥ 60 –±–æ–ª–≥–æ–≤
    dwell = int(os.getenv("DWELL_SEC", "60")) 
    # Caak –≥—ç—Ö –º—ç—Ç —Å–∞–π—Ç—É—É–¥ —Ö—ç—Ç —à“Ø“Ø—Ö–≥“Ø–π–Ω —Ç—É–ª–¥ default score 3 –±–∞–π—Ö —Ö—ç—Ä—ç–≥—Ç—ç–π
    min_score = int(os.getenv("ADS_MIN_SCORE", "3"))
    
    ads_only = os.getenv("ADS_ONLY", "1") == "1"
    headless = os.getenv("HEADLESS", "1") == "1"
    
    print(f"‚è≥ Starting: {name} (Dwell: {dwell}s, Score: {min_score}, Headless: {headless})...")
    
    try:
        # –ë“Ø—Ö –º–æ–¥—É–ª—å 'scrape_{name_prefix}' —ç—Å–≤—ç–ª —Å—Ç–∞–Ω–¥–∞—Ä—Ç 'scrape' —Ñ—É–Ω–∫—Ü—Ç—ç–π –≥—ç–∂ “Ø–∑–Ω—ç.
        prefix = name.split('_')[0] # "ikon" from "ikon_mn"
        func_name = f"scrape_{prefix}"
        
        if hasattr(mod, func_name):
            scraper_func = getattr(mod, func_name)
            
            # 2. –¢–æ—Ö–∏—Ä–≥–æ–æ–Ω—É—É–¥—ã–≥ —Ñ—É–Ω–∫—Ü —Ä“Ø“Ø –¥–∞–º–∂—É—É–ª–∞—Ö (–≠–ù–≠ –•–≠–°–≠–ì–¢ –ó–ê–°–í–ê–† –û–†–°–û–ù)
            results = scraper_func(
                output_dir="./banner_screenshots",
                dwell_seconds=dwell,  # Rotating ads –±–∞—Ä–∏—Ö —Ö—É–≥–∞—Ü–∞–∞
                headless=headless,    # Server –¥—ç—ç—Ä True –±–∞–π—Ö —ë—Å—Ç–æ–π
                ads_only=ads_only,    # –ó”©–≤—Ö”©–Ω –∑–∞—Ä –∞–≤–∞—Ö —ç—Å—ç—Ö
                min_score=min_score   # –ó–∞—Ä —Ç–∞–Ω–∏—Ö –±–æ—Å–≥–æ –æ–Ω–æ–æ
            )
            
        elif hasattr(mod, "scrape"):
            # –•—ç—Ä—ç–≤ —Ö—É—É—á–∏–Ω 'scrape' –Ω—ç—Ä—Ç—ç–π —Ñ—É–Ω–∫—Ü –±–∞–π–≤–∞–ª (fallback)
            results = mod.scrape()
        else:
            print(f"‚ö† Warning: No scrape function found for {name}")
            
        print(f"‚úÖ Finished: {name} (Found {len(results)} items)")
        return {name: results}
        
    except Exception as e:
        print(f"‚ùå Error in {name}: {str(e)}")
        traceback.print_exc()
        return {name: []}

def scrape_all_sites() -> Dict[str, List]:
    """
    Runs all scrapers in parallel using ThreadPoolExecutor.
    """
    all_results = {}
    
    # 3. Worker-–∏–π–Ω —Ç–æ–æ–≥ –∞—é—É–ª–≥“Ø–π–≥—ç—ç—Ä —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö
    # .env-–¥ –±–∞–π—Ö–≥“Ø–π –±–æ–ª default –Ω—å 2 (t3.medium –¥—ç—ç—Ä RAM —Ö—ç–º–Ω—ç–Ω—ç)
    MAX_WORKERS = int(os.getenv("MAX_WORKERS", "2"))
    
    print(f"üöÄ Launching parallel scraper with {MAX_WORKERS} workers...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Submit all tasks
        futures = [executor.submit(_scrape_wrapper, site) for site in SITES_CONFIG]
        
        # Collect results as they finish
        for future in concurrent.futures.as_completed(futures):
            try:
                data = future.result()
                all_results.update(data)
            except Exception as exc:
                print(f"‚ùå Critical Thread Error: {exc}")

    return all_results